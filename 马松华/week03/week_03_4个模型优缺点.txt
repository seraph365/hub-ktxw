正则表达式|TFIDF |BERT|大语言模型 优缺点对比

| 方法                  | 核心思想                                             | 时代              | 优点                                                                                                     | 缺点                                                                                              |
| :------------------- | :-------------------------------------------------- | :--------------- | :----------------------------------------------------------- --------------------------------------------| :----------------------------------------------------------------------------------------------- |
|  正则表达式            | 基于人工定义的规则和模式进行匹配                         | 规则时代           | 1. 极高精确度（规则明确时） 2. 无需训练数据 3. 完全可控、可解释 4. 计算开销极低                                    | 1. 极低召回率（泛化能力为零） 2. 依赖专家知识，耗时费力 3. 难以维护 4. 无法处理语义变化                      |
|  TF-IDF              | 将文本转为词频统计向量，用机器学习模型分类                 | 统计机器学习时代    | 1. 不需要大量数据 2. 可解释性较好 3. 计算效率较高 4. 不依赖深度学习硬件                                           | 1. 忽略词序和语义（“狗咬人”和“人咬狗”一样） 2. 特征向量高维稀疏 3. 难以处理未登录词(OOV) 4. 性能天花板较低     |
|  BERT                | 使用深度Transformer模型获取上下文词义，微调进行分类        | 深度学习时代       | 1. **强大性能**（曾经的SOTA） 2. **理解上下文和语义** 3. 强大的泛化能力 4. 免于繁琐的特征工程                       | 1. 需要大量标注数据 2. 计算资源需求大（训练/微调） 3. 模型庞大，推理稍慢 4. 黑盒模型，可解释性差              |
|  大语言模型 (LLM)      | 利用超大规模预训练模型的指令理解与上下文学习能力            | 大模型时代         | 1. ****卓越性能**（常为新SOTA） 2. **零样本/少样本**学习 3. **无需微调**（Prompt即可） 4. 通用性强，一模型多任务     | 1. **极其昂贵**（API调用或训练成本） 2. 数据隐私和安全风险 3. **Prompt工程**是门学问 4. 最大黑盒，可控性最差 |
| :------------------- | :-------------------------------------------------- | :--------------- | :----------------------------------------------------------- --------------------------------------------| :----------------------------------------------------------------------------------------------- |
