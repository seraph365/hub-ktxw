
"""
 Author: Marky
 Time: 2025/8/29 08:14 
 Description:
"""

=========================================================================================================
RNN(循环神经网络)计算过程：
是一类专门用于处理序列数据的神经网络架构。与传统的前馈神经网络不同，RNN具有"记忆"能力，能够捕获数据中的时间动态信息

一、RNN的核心思想
循环结构:
    RNN的核心特点是它具有循环连接，使网络能够保持对先前信息的记忆
    h_t = f(W * x_t + U * h_{t-1} + b)
    其中：
    h_t 是当前时间步的隐藏状态
    x_t 是当前时间步的输入
    h_{t-1} 是前一个时间步的隐藏状态
    W, U, b 是网络参数
    f 是激活函数（通常是tanh或ReLU）

二、RNN的数学表达
2.1 前向传播
对于时间步t：
计算隐藏状态：h_t = tanh(W_{xh} * x_t + W_{hh} * h_{t-1} + b_h)
计算输出：y_t = W_{hy} * h_t + b_y
2.2 随时间展开
通过将RNN按时间步展开，可以更清楚地看到其结构：
时间步 0: h_0 = tanh(W_{xh} * x_0 + W_{hh} * h_{-1} + b_h)
时间步 1: h_1 = tanh(W_{xh} * x_1 + W_{hh} * h_0 + b_h)
时间步 2: h_2 = tanh(W_{xh} * x_2 + W_{hh} * h_1 + b_h)
...
时间步 t: h_t = tanh(W_{xh} * x_t + W_{hh} * h_{t-1} + b_h)



=========================================================================================================
GRU 计算过程：
GRU的核心是通过两个门控（更新门和重置门）来决定哪些信息需要被保留、哪些需要被遗忘，从而有效地捕捉序列中的长期依赖关系。其计算过程是一步一步进行的，对于序列中的每个时间步 t，都会接收两个输入：
当前时间步的输入 x_t
前一个时间步的隐藏状态 h_{t-1}（代表了到上一时刻为止模型所记忆的信息）
并产生两个输出：
当前时间步的隐藏状态 h_t（作为下一时间步的输入，也是当前时间步的主要输出）
当前时间步的输出 y_t（通常由 h_t 经过一个简单的变换，如全连接层，得到）

=========================================================================================================
LSTM 计算过程：
明确设计用来解决传统RNN的梯度消失和梯度爆炸问题，从而能够有效地学习、记忆和利用序列数据中的长期依赖关系LSTM的关键在于两条信息传递路径：
隐藏状态（hₜ）： 与传统RNN类似，是当前时间步的输出，并传递给下一时间步。它承载了短期记忆。
细胞状态（Cₜ）： 这是LSTM的精髓所在。它像一条“信息高速公路”，贯穿所有时间步。信息可以在这条公路上以几乎不变的形式轻松地流动很长距离，承载着长期记忆
门（Gate）： 是一种让信息选择性地通过的结构，由一个Sigmoid层和一个点乘操作组成。
Sigmoid层：输出0到1之间的值，描述每个部分应该通过多少。0代表“不让任何东西通过”，1代表“让所有东西通过”。
点乘操作：应用Sigmoid输出的权重来控制信息。

3. LSTM的计算步骤详解（结合图表）
对于每个时间步 t，LSTM接收以下输入：
当前输入 x_t
上一时间步的隐藏状态 h_{t-1}
上一时间步的细胞状态 C_{t-1}
并通过以下步骤计算输出：

第1步：决定遗忘什么 - 遗忘门（Forget Gate）
作用：查看 h_{t-1} 和 x_t，并为细胞状态 C_{t-1} 中的每个数字输出一个0到1之间的数，决定哪些信息应该被丢弃或保留。
公式： f_t = σ(W_f · [h_{t-1}, x_t] + b_f)
[h_{t-1}, x_t] 表示将两个向量连接（concatenate）。
结果 f_t： 接近0意味着“完全忘记”，接近1意味着“完全保留”。

第2步：决定存储什么 - 输入门（Input Gate） & 候选值
作用：决定要在细胞状态中添加哪些新信息。
a) 输入门： 决定哪些新信息值得更新。
公式： i_t = σ(W_i · [h_{t-1}, x_t] + b_i)
b) 候选细胞状态： 一个包含潜在新信息的向量。
公式： \tilde{C}_t = tanh(W_C · [h_{t-1}, x_t] + b_C)

第3步：更新细胞状态
作用：将旧的细胞状态 C_{t-1} 更新为新的细胞状态 C_t。
公式： C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t
这是LSTM最核心的方程！
过程：
忘记： 将旧的状态 C_{t-1} 与遗忘门 f_t 相乘，丢弃掉我们决定忘记的信息。
添加： 将输入门 i_t 与候选状态 \tilde{C}_t 相乘，得到我们决定添加的新信息。
结合： 将以上两步的结果相加，形成新的、更新后的细胞状态 C_t

第4步：决定输出什么 - 输出门（Output Gate）
作用：基于细胞状态，决定下一个隐藏状态应该是什么。隐藏状态包含了当前时间步的输出信息。
a) 输出门： 决定细胞状态中哪些部分将用于输出。
公式： o_t = σ(W_o · [h_{t-1}, x_t] + b_o)
b) 最终隐藏状态： 将细胞状态通过tanh（使其值在-1到1之间）并与输出门相乘。
公式： h_t = o_t \odot tanh(C_t)
输出： h_t 将作为当前时间步的输出（y_t 通常由 h_t 变换得到），并传递给下一个时间步。
