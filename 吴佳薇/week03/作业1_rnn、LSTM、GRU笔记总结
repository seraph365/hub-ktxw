'''rnn、LSTM、GRU笔记总结
1. RNN (循环神经网络)
核心思想：引入“隐藏状态” h_t 作为记忆，捕获序列之前的信息。

计算过程 (一个时间步)：

更新隐藏状态：
h_t = tanh(W_xh * x_t + W_hh * h_{t-1} + b_h)

计算输出：
o_t = W_hy * h_t + b_y

问题：tanh 激活函数和循环权重 W_hh 的连续相乘会导致梯度消失/爆炸，难以学习长期依赖。

2. LSTM (长短期记忆网络)
核心思想：引入“门控机制”和“细胞状态” C_t（一条信息高速公路）来精确控制记忆。

三个门（每个门都是一个神经网络层，输出值在0到1之间）：

遗忘门 (f_t)：“哪些旧信息需要丢弃？”

输入门 (i_t)：“哪些新信息需要存入？”

输出门 (o_t)：“当前要输出什么？”

计算过程 (一个时间步)：

计算门和候选值：
f_t = σ(W_f · [h_{t-1}, x_t] + b_f)
i_t = σ(W_i · [h_{t-1}, x_t] + b_i)
o_t = σ(W_o · [h_{t-1}, x_t] + b_o)
C̃_t = tanh(W_C · [h_{t-1}, x_t] + b_C)

更新细胞状态：
C_t = f_t * C_{t-1} + i_t * C̃_t

计算隐藏状态（输出）：
h_t = o_t * tanh(C_t)

3. GRU (门控循环单元)
核心思想：LSTM的变体，结构更简单。将LSTM的遗忘门和输入门合并为更新门，并混合了细胞状态和隐藏状态。

两个门：

更新门 (z_t)：平衡旧信息和新信息的比例。

重置门 (r_t)：控制过去信息对计算新候选状态的影响。

计算过程 (一个时间步)：

计算门：
z_t = σ(W_z · [h_{t-1}, x_t] + b_z)
r_t = σ(W_r · [h_{t-1}, x_t] + b_r)

计算候选状态：
h̃_t = tanh(W · [r_t * h_{t-1}, x_t] + b_h)

更新隐藏状态（兼输出）：
h_t = (1 - z_t) * h_{t-1} + z_t * h̃_t

GRU是很好的默认起点，计算效率高。LSTM在处理非常长的序列依赖时可能更强大。基本RNN现在已较少用于实际应用。
'''
