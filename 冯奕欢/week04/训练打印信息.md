```commandline
D:\NLP\software\miniconda3\envs\py312\python.exe D:\NLP\project\nlp\week04\train\train_bert.py 
D:\NLP\software\miniconda3\envs\py312\Lib\site-packages\requests\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
0       很快，好吃，味道足，量大
1       没有送水没有送水没有送水
2           非常快，态度好。
3    方便，快捷，味道可口，快递给力
4       菜味道很棒！送餐很及时！
Name: review, dtype: object
0    1
1    1
2    1
3    1
4    1
Name: label, dtype: int64
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../assets/models/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Epoch [1/1], Step [1/150], Loss: 0.8091
Epoch [1/1], Step [2/150], Loss: 0.6807
Epoch [1/1], Step [3/150], Loss: 0.6142
Epoch [1/1], Step [4/150], Loss: 0.6017
Epoch [1/1], Step [5/150], Loss: 0.5667
Epoch [1/1], Step [6/150], Loss: 0.4533
Epoch [1/1], Step [7/150], Loss: 0.5913
Epoch [1/1], Step [8/150], Loss: 0.4823
Epoch [1/1], Step [9/150], Loss: 0.4107
Epoch [1/1], Step [10/150], Loss: 0.4007
Epoch [1/1], Step [11/150], Loss: 0.3854
Epoch [1/1], Step [12/150], Loss: 0.3625
Epoch [1/1], Step [13/150], Loss: 0.3054
Epoch [1/1], Step [14/150], Loss: 0.4076
Epoch [1/1], Step [15/150], Loss: 0.3426
Epoch [1/1], Step [16/150], Loss: 0.5674
Epoch [1/1], Step [17/150], Loss: 0.2011
Epoch [1/1], Step [18/150], Loss: 0.2446
Epoch [1/1], Step [19/150], Loss: 0.3018
Epoch [1/1], Step [20/150], Loss: 0.3540
Epoch [1/1], Step [21/150], Loss: 0.4058
Epoch [1/1], Step [22/150], Loss: 0.2579
Epoch [1/1], Step [23/150], Loss: 0.2536
Epoch [1/1], Step [24/150], Loss: 0.1647
Epoch [1/1], Step [25/150], Loss: 0.3878
Epoch [1/1], Step [26/150], Loss: 0.1895
Epoch [1/1], Step [27/150], Loss: 0.4324
Epoch [1/1], Step [28/150], Loss: 0.4164
Epoch [1/1], Step [29/150], Loss: 0.1928
Epoch [1/1], Step [30/150], Loss: 0.4168
Epoch [1/1], Step [31/150], Loss: 0.3241
Epoch [1/1], Step [32/150], Loss: 0.2232
Epoch [1/1], Step [33/150], Loss: 0.3792
Epoch [1/1], Step [34/150], Loss: 0.2220
Epoch [1/1], Step [35/150], Loss: 0.3261
Epoch [1/1], Step [36/150], Loss: 0.3619
Epoch [1/1], Step [37/150], Loss: 0.4379
Epoch [1/1], Step [38/150], Loss: 0.3398
Epoch [1/1], Step [39/150], Loss: 0.2693
Epoch [1/1], Step [40/150], Loss: 0.3696
Epoch [1/1], Step [41/150], Loss: 0.3002
Epoch [1/1], Step [42/150], Loss: 0.3317
Epoch [1/1], Step [43/150], Loss: 0.3037
Epoch [1/1], Step [44/150], Loss: 0.3788
Epoch [1/1], Step [45/150], Loss: 0.2540
Epoch [1/1], Step [46/150], Loss: 0.3643
Epoch [1/1], Step [47/150], Loss: 0.1756
Epoch [1/1], Step [48/150], Loss: 0.2921
Epoch [1/1], Step [49/150], Loss: 0.3308
Epoch [1/1], Step [50/150], Loss: 0.2548
Epoch [1/1], Step [51/150], Loss: 0.3221
Epoch [1/1], Step [52/150], Loss: 0.3401
Epoch [1/1], Step [53/150], Loss: 0.4121
Epoch [1/1], Step [54/150], Loss: 0.1689
Epoch [1/1], Step [55/150], Loss: 0.2145
Epoch [1/1], Step [56/150], Loss: 0.3269
Epoch [1/1], Step [57/150], Loss: 0.1642
Epoch [1/1], Step [58/150], Loss: 0.3569
Epoch [1/1], Step [59/150], Loss: 0.3769
Epoch [1/1], Step [60/150], Loss: 0.2515
Epoch [1/1], Step [61/150], Loss: 0.1885
Epoch [1/1], Step [62/150], Loss: 0.1590
Epoch [1/1], Step [63/150], Loss: 0.1882
Epoch [1/1], Step [64/150], Loss: 0.2064
Epoch [1/1], Step [65/150], Loss: 0.2970
Epoch [1/1], Step [66/150], Loss: 0.2652
Epoch [1/1], Step [67/150], Loss: 0.2243
Epoch [1/1], Step [68/150], Loss: 0.3121
Epoch [1/1], Step [69/150], Loss: 0.2339
Epoch [1/1], Step [70/150], Loss: 0.3380
Epoch [1/1], Step [71/150], Loss: 0.1879
Epoch [1/1], Step [72/150], Loss: 0.2116
Epoch [1/1], Step [73/150], Loss: 0.2965
Epoch [1/1], Step [74/150], Loss: 0.1993
Epoch [1/1], Step [75/150], Loss: 0.2853
Epoch [1/1], Step [76/150], Loss: 0.2436
Epoch [1/1], Step [77/150], Loss: 0.2132
Epoch [1/1], Step [78/150], Loss: 0.3366
Epoch [1/1], Step [79/150], Loss: 0.2325
Epoch [1/1], Step [80/150], Loss: 0.1504
Epoch [1/1], Step [81/150], Loss: 0.1985
Epoch [1/1], Step [82/150], Loss: 0.2391
Epoch [1/1], Step [83/150], Loss: 0.2481
Epoch [1/1], Step [84/150], Loss: 0.1686
Epoch [1/1], Step [85/150], Loss: 0.1892
Epoch [1/1], Step [86/150], Loss: 0.1742
Epoch [1/1], Step [87/150], Loss: 0.4171
Epoch [1/1], Step [88/150], Loss: 0.1908
Epoch [1/1], Step [89/150], Loss: 0.3589
Epoch [1/1], Step [90/150], Loss: 0.3337
Epoch [1/1], Step [91/150], Loss: 0.3025
Epoch [1/1], Step [92/150], Loss: 0.0889
Epoch [1/1], Step [93/150], Loss: 0.4103
Epoch [1/1], Step [94/150], Loss: 0.2001
Epoch [1/1], Step [95/150], Loss: 0.3721
Epoch [1/1], Step [96/150], Loss: 0.2330
Epoch [1/1], Step [97/150], Loss: 0.1983
Epoch [1/1], Step [98/150], Loss: 0.3171
Epoch [1/1], Step [99/150], Loss: 0.2446
Epoch [1/1], Step [100/150], Loss: 0.2485
Epoch [1/1], Step [101/150], Loss: 0.3426
Epoch [1/1], Step [102/150], Loss: 0.2450
Epoch [1/1], Step [103/150], Loss: 0.2985
Epoch [1/1], Step [104/150], Loss: 0.1594
Epoch [1/1], Step [105/150], Loss: 0.1828
Epoch [1/1], Step [106/150], Loss: 0.2649
Epoch [1/1], Step [107/150], Loss: 0.1983
Epoch [1/1], Step [108/150], Loss: 0.1885
Epoch [1/1], Step [109/150], Loss: 0.1619
Epoch [1/1], Step [110/150], Loss: 0.1682
Epoch [1/1], Step [111/150], Loss: 0.2300
Epoch [1/1], Step [112/150], Loss: 0.3463
Epoch [1/1], Step [113/150], Loss: 0.2288
Epoch [1/1], Step [114/150], Loss: 0.2796
Epoch [1/1], Step [115/150], Loss: 0.3573
Epoch [1/1], Step [116/150], Loss: 0.2959
Epoch [1/1], Step [117/150], Loss: 0.3707
Epoch [1/1], Step [118/150], Loss: 0.3803
Epoch [1/1], Step [119/150], Loss: 0.3112
Epoch [1/1], Step [120/150], Loss: 0.1061
Epoch [1/1], Step [121/150], Loss: 0.2529
Epoch [1/1], Step [122/150], Loss: 0.2553
Epoch [1/1], Step [123/150], Loss: 0.1768
Epoch [1/1], Step [124/150], Loss: 0.3703
Epoch [1/1], Step [125/150], Loss: 0.2665
Epoch [1/1], Step [126/150], Loss: 0.2491
Epoch [1/1], Step [127/150], Loss: 0.3850
Epoch [1/1], Step [128/150], Loss: 0.1528
Epoch [1/1], Step [129/150], Loss: 0.2334
Epoch [1/1], Step [130/150], Loss: 0.4385
Epoch [1/1], Step [131/150], Loss: 0.1963
Epoch [1/1], Step [132/150], Loss: 0.3383
Epoch [1/1], Step [133/150], Loss: 0.1962
Epoch [1/1], Step [134/150], Loss: 0.2299
Epoch [1/1], Step [135/150], Loss: 0.3358
Epoch [1/1], Step [136/150], Loss: 0.2532
Epoch [1/1], Step [137/150], Loss: 0.1956
Epoch [1/1], Step [138/150], Loss: 0.3308
Epoch [1/1], Step [139/150], Loss: 0.2474
Epoch [1/1], Step [140/150], Loss: 0.2791
Epoch [1/1], Step [141/150], Loss: 0.2119
Epoch [1/1], Step [142/150], Loss: 0.2635
Epoch [1/1], Step [143/150], Loss: 0.3257
Epoch [1/1], Step [144/150], Loss: 0.2596
Epoch [1/1], Step [145/150], Loss: 0.1819
Epoch [1/1], Step [146/150], Loss: 0.3442
Epoch [1/1], Step [147/150], Loss: 0.4229
Epoch [1/1], Step [148/150], Loss: 0.2348
Epoch [1/1], Step [149/150], Loss: 0.2900
Epoch [1/1], Step [150/150], Loss: 0.1460
模型已保存至 ../assets/weights/bert-big.pt
Step [1/38], Loss: 0.3581
[0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0
 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0]  ==  [1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0
 1 1 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 0 0 1 1 0]
Predict accuracy on test set: 0.8750
Step [2/38], Loss: 0.2020
[0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0
 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 0 0]  ==  [0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0
 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0]
Predict accuracy on test set: 0.9375
Step [3/38], Loss: 0.1958
[0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0
 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0]  ==  [0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0
 1 1 0 0 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1 0 0]
Predict accuracy on test set: 0.9062
Step [4/38], Loss: 0.1838
[0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1
 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1]  ==  [0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1
 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0]
Predict accuracy on test set: 0.9375
Step [5/38], Loss: 0.2996
[1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0
 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 0 0 0]  ==  [0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0
 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 0 0 0]
Predict accuracy on test set: 0.8750
Step [6/38], Loss: 0.1838
[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 0 1
 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1]  ==  [0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1
 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1]
Predict accuracy on test set: 0.9375
Step [7/38], Loss: 0.2205
[0 0 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0
 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1]  ==  [0 0 0 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0
 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 1]
Predict accuracy on test set: 0.9219
Step [8/38], Loss: 0.3041
[0 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 1 0 0
 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0]  ==  [0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0
 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1]
Predict accuracy on test set: 0.8750
Step [9/38], Loss: 0.3169
[0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 1
 0 1 1 0 0 0 0 1 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0]  ==  [0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1
 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 0 0 0]
Predict accuracy on test set: 0.9219
Step [10/38], Loss: 0.2227
[1 0 0 1 0 0 1 1 0 1 1 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1
 0 1 0 0 1 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0]  ==  [1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1
 0 1 0 0 1 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0]
Predict accuracy on test set: 0.9219
Step [11/38], Loss: 0.2365
[1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0
 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0]  ==  [1 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 0 1 1 0 0 0 0 0 1 0 0 0
 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0]
Predict accuracy on test set: 0.9219
Step [12/38], Loss: 0.2084
[0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 0
 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1]  ==  [0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 1 1 0
 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1]
Predict accuracy on test set: 0.9375
Step [13/38], Loss: 0.2175
[1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0
 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1]  ==  [1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 1 0 0 0 1 0
 0 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1]
Predict accuracy on test set: 0.9375
Step [14/38], Loss: 0.2696
[0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1
 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0]  ==  [0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 1 1
 0 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0]
Predict accuracy on test set: 0.9219
Step [15/38], Loss: 0.1940
[0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1
 1 0 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0]  ==  [0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1
 1 0 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0]
Predict accuracy on test set: 0.9375
Step [16/38], Loss: 0.1403
[0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1
 0 1 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0 0 0]  ==  [0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1
 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0 0 0]
Predict accuracy on test set: 0.9531
Step [17/38], Loss: 0.3010
[1 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0
 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 1]  ==  [1 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0
 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 1 1]
Predict accuracy on test set: 0.8906
Step [18/38], Loss: 0.4943
[0 1 0 0 1 1 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0 1 0 1 0 0 0
 1 1 0 0 1 0 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 1 1]  ==  [0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0
 1 1 0 0 1 0 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0 0 0 1 1]
Predict accuracy on test set: 0.7812
Step [19/38], Loss: 0.3203
[0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 1 1 0 0 1 0 1 1 1 0 0 0 0 0 1 0
 0 0 1 1 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0]  ==  [0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0
 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0]
Predict accuracy on test set: 0.8750
Step [20/38], Loss: 0.3188
[0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1
 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1]  ==  [1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 0
 0 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1]
Predict accuracy on test set: 0.8594
Step [21/38], Loss: 0.1864
[1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 1
 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1]  ==  [1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 1 1 0 0 0 0 1 1
 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 1]
Predict accuracy on test set: 0.9375
Step [22/38], Loss: 0.1976
[1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0
 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0]  ==  [1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0
 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0]
Predict accuracy on test set: 0.9531
Step [23/38], Loss: 0.3298
[0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 0 0
 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0]  ==  [1 0 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 0 0 0 1
 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0]
Predict accuracy on test set: 0.8906
Step [24/38], Loss: 0.3754
[0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0
 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0]  ==  [0 1 1 1 0 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0
 0 1 1 0 1 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0]
Predict accuracy on test set: 0.8281
Step [25/38], Loss: 0.3231
[0 1 1 0 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 0
 0 0 1 0 0 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0]  ==  [0 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 1 1
 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0]
Predict accuracy on test set: 0.8906
Step [26/38], Loss: 0.2263
[0 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0
 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0]  ==  [0 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0
 1 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0]
Predict accuracy on test set: 0.9062
Step [27/38], Loss: 0.3116
[0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0
 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 0 0 1 1 0 0 1]  ==  [0 1 1 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0
 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1]
Predict accuracy on test set: 0.8906
Step [28/38], Loss: 0.1824
[0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1
 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 1]  ==  [1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1
 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 1]
Predict accuracy on test set: 0.9219
Step [29/38], Loss: 0.4013
[1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1
 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1]  ==  [0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1
 1 0 1 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1]
Predict accuracy on test set: 0.7812
Step [30/38], Loss: 0.2352
[0 0 1 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 1 0 0 0
 1 1 0 0 1 1 0 0 0 0 1 1 1 0 1 1 0 0 0 1 0 0 0 0 0 0 1]  ==  [0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0
 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 1 0 0 1]
Predict accuracy on test set: 0.8750
Step [31/38], Loss: 0.3027
[0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0
 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0]  ==  [0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 0 0 0 0 1 0 0
 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 1]
Predict accuracy on test set: 0.9062
Step [32/38], Loss: 0.3134
[1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0
 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 1]  ==  [1 0 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 0 1 0 0 0 0 0 0 0
 0 0 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1]
Predict accuracy on test set: 0.8750
Step [33/38], Loss: 0.2059
[1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0
 1 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1]  ==  [1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0
 1 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1 1]
Predict accuracy on test set: 0.9062
Step [34/38], Loss: 0.3438
[1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0
 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0]  ==  [0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0
 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0]
Predict accuracy on test set: 0.8750
Step [35/38], Loss: 0.2479
[0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0
 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1]  ==  [0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
 1 1 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 0 0 1]
Predict accuracy on test set: 0.9375
Step [36/38], Loss: 0.2557
[0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 0 1 0
 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0]  ==  [0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 1 1 0 1 0 1 1 0
 0 0 1 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 1 1 0 0]
Predict accuracy on test set: 0.8906
Step [37/38], Loss: 0.1712
[0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1
 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0]  ==  [0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1
 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0]
Predict accuracy on test set: 0.9375
Step [38/38], Loss: 0.1364
[1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 0 0]  ==  [1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0]
Predict accuracy on test set: 0.9667
Accuracy on test set: 0.9025
```