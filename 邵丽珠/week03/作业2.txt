BERT（预训练模型）
优点：
强大的语义理解能力：能深刻理解上下文、一词多义、语法结构等。它是真正能“读懂”语言的方法。
出色的泛化能力：经过预训练后，只需少量标注数据微调，就能在特定任务上取得极好效果。
缺点：
需大量数据和算力：预训练成本极高，微调也需要相当的资源。
黑盒模型：决策过程难以解释，不知道为什么模型会做出某个预测。
部署和推理成本高：模型参数量大（数亿甚至数十亿），推理速度慢，对硬件要求高。

Prompt（提示学习）
优点：
无需微调或少样本：极大降低了资源需求和使用门槛，甚至可以零样本（Zero-shot） 学习。
极其灵活：同一个模型，通过不同的Prompt，可以完成分类、生成、翻译、推理等无数任务。
更接近人类交互方式：用自然语言“指导”模型，而不是用数据“训练”模型。
缺点：
严重依赖基座模型的能力：如果基座模型（如GPT）本身能力不强，Prompt效果会很差。
提示工程（Prompt Engineering）是门艺术：Prompt的设计需要技巧和经验，细微的改动可能导致结果差异巨大。
结果具有不确定性：模型可能会生成意想不到的内容，可控性不如传统方法。

Regex（正则表达式）
优点：
精确可控：规则由你完全定义，匹配结果100%确定。
无需训练：直接使用，不需要任何数据。
速度极快：匹配过程是确定性计算，效率极高。
可解释性极强：规则是人写的，完全知道为什么匹配或不匹配。
缺点：
无法泛化：只能匹配预设的模式。无法理解“帮我订票”和“我想购买一张票”是同一个意思。
维护成本高：规则会越来越复杂，难以维护（俗称“正则地狱”）。
毫无语义理解能力：它只认识字符，不认识语言。

TF-IDF（词频-逆文档频率）
优点：
无监督，简单有效：不需要标注数据，容易理解和实现。
可解释性较好：可以列出哪些关键词对文档最重要。
计算成本相对较低（相对于深度学习）。
缺点：
忽略词序和语义：“猫吃鱼”和“鱼吃猫”的向量表示是完全一样的（词袋模型缺陷）。
无法处理一词多义和同义词：“苹果”手机和“苹果”水果无法区分；“电脑”和“计算机”被视为完全不同的词。
向量稀疏且高维：词汇表很大时，向量维度会非常高，且大部分元素为0。

3.

4.