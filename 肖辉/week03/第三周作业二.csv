1. 正则表达式
作用：
    正则表达式是一种用于匹配字符串中字符组合的模式。它本身不是一个“模型”，而是一种强大、精确的文本匹配和操作工具。在NLP中，它常用于模式查找、文本清洗、提取结构化信息等。

工作原理：
    通过预定义的一系列特殊字符（如 ., *, +, ?, \d, |, [] 等）来编写规则，描述所需的字符串模式。

优点：
    精确无误：规则由开发者完全控制，只要模式写对，匹配结果100%准确，没有歧义。
    速度快：匹配过程是确定性算法，效率极高，通常在微秒级别完成。
    无需训练：不需要任何训练数据，开箱即用。
    可解释性强：规则清晰可见，任何人都可以理解和验证其逻辑。

缺点：
    僵化且脆弱：无法处理任何模式外的变体。例如，写规则提取“电话”，就无法处理“Tel.”、“手机号”等未预先定义的写法。
    无法理解语义：完全基于字符表面形式，无法理解同义词、上下文和意图。例如，无法理解“苹果”指的是水果还是公司。
    开发和维护成本高：覆盖所有可能的模式变体需要编写大量复杂的规则，难以维护。

2. TF-IDF
作用：
    TF-IDF是一种统计方法，用于评估一个单词对于一个文档集或语料库中的其中一份文档的重要程度。它通常将文本转换为固定长度的数值向量（词袋模型），以便计算机能够处理。

工作原理：
    TF：词频，一个词在当前文档中出现的次数越多，越重要。
    IDF：逆文档频率，一个词在整个语料库中出现的文档越少，越能代表该文档的特色。
    TF-IDF = TF * IDF：综合两者，过滤掉常见词（如“的”、“是”），保留重要的关键词。

优点：
    简单有效：概念简单，实现容易，在很多传统任务上效果不错。
    无监督：不需要标注数据，仅从文本本身就能计算。
    可解释：可以查看每个文档中TF-IDF分数最高的词，大致了解文档主题。

缺点：
    词袋模型缺陷：完全忽略词序、语法和上下文信息。“狗咬人”和“人咬狗”的向量表示是一样的。
    无法捕获语义：无法理解同义词（“电脑”和“计算机”被视为完全不同的词）和多义词（“苹果”在不同上下文中的含义无法区分）。
    维度灾难：词汇表很大，导致向量维度极高且稀疏，计算和存储效率低。


3. BERT
作用：
    BERT是基于深度学习的预训练语言模型，它的核心是为自然语言理解任务提供深层的、上下文相关的词向量表示。它彻底改变了NLP领域。

工作原理：
    Transformer架构：使用自注意力机制，可以并行处理所有单词并计算它们之间的相互关系权重。
    双向上下文：与之前模型（从左到右）不同，BERT同时考虑一个词左右两侧的上下文，从而更好地理解词义。
    预训练+微调：
        预训练：在海量无标注文本上通过“掩码语言模型”和“下一句预测”任务进行训练，学习通用的语言规律。
        微调：在特定下游任务（如分类、问答）上用少量标注数据对预训练模型进行微调，使其适应特定任务。

优点：
    深度理解语义和上下文：能出色处理一词多义、同义词、复杂语言现象。例如，能根据上下文区分“苹果很甜”和“苹果发布了新手机”中的“苹果”。
    强大的泛化能力：经过预训练后，只需少量标注数据微调就能在多个任务上取得极佳效果。
    通用性强：一个模型架构通过微调可以用于分类、问答、命名实体识别等多种任务。

缺点：
    计算资源巨大：训练和推理需要大量的GPU计算力和内存，成本高昂。
    黑盒模型：决策过程难以解释，不像规则或TF-IDF那样透明。
    需要微调：虽然不需要从头训练，但仍需要任务特定的数据和微调步骤。


4. 大语言模型
作用：
    LLMs是超大规模的基于Transformer架构的预训练语言模型。它们不仅是理解语言，更侧重于生成高质量、连贯、创造性的人类语言。

工作原理：
    海量参数：拥有数百亿甚至万亿级的参数，存储在极其庞大的神经网络中。
    海量数据：在互联网规模的文本和代码上进行训练。
    生成能力：通常采用“自回归”方式，根据给定的上文（提示），逐个 token（词元）地预测生成下文。

优点：
    强大的涌现能力：表现出在训练数据中未明确设定的能力，如推理、编程、创作等。
    卓越的生成能力：能够生成流畅、连贯、富有创造性的长文本。
    通用任务解决：通过“提示”几乎可以处理任何NLP任务，而无需微调（零样本/少样本学习）。
    对话交互：能够进行多轮、开放域的对话，体验接近人类。

缺点：
    极高的计算成本：训练和部署成本是天文数字，只有大公司能承担。
    幻觉问题：可能会生成看似合理但完全不正确或不存在的信息。
    不可控性：输出可能包含偏见、有害内容，且难以完全控制。
    黑盒性极强：其内部工作机制和决策逻辑几乎完全不可解释。
